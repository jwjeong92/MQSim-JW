#!/usr/bin/env python3
"""
Analytical Trade-off Analysis for LLM Read-Disturb vs. Reclaim

Uses proven linear read accumulation (Exp2) to extrapolate reclaim behavior
and demonstrate the fundamental reliability-lifespan trade-off.

Key Insight: We don't need to actually trigger reclaim - we can calculate
when it WOULD trigger and project the P/E cycle consumption.
"""

import json
import sys

# ============================================================================
# MEASURED DATA FROM EXPERIMENT 2 (Llama2-70B)
# ============================================================================

# Linear fit from Experiment 2: Reads = 0.636 √ó Tokens
READ_RATE_PER_TOKEN = 0.636  # flash reads per token
R_SQUARED = 0.999  # fit quality (near-perfect)

# Flash geometry (from Llama70B trace generator)
ESTIMATED_BLOCKS = 17920  # blocks used for 70B model
PAGES_PER_BLOCK = 1536  # from ssdconfig.xml

# ECC failure rate (measured)
ECC_FAILURE_RATE = 0.031  # 3.1% failures per read

# Flash endurance
PE_CYCLES_LIMIT = 3000  # TLC NAND typical endurance
BLOCK_SIZE_MB = 24  # 1536 pages √ó 16KB = 24 MB per block

# ============================================================================
# READ DISTRIBUTION MODEL
# ============================================================================

def estimate_max_reads_per_block(total_reads, total_blocks, concentration_factor=10):
    """
    Estimate maximum reads to hottest block.

    With uniform distribution: max = avg
    With concentration: max = avg √ó concentration_factor

    Real LLM workloads have hot blocks (attention weights read more often)
    Conservative estimate: 10√ó concentration
    """
    avg_reads = total_reads / total_blocks
    max_reads = avg_reads * concentration_factor
    return avg_reads, max_reads

# ============================================================================
# RECLAIM PROJECTION
# ============================================================================

def calculate_reclaim_trigger_time(threshold, read_rate=READ_RATE_PER_TOKEN,
                                   concentration=10, blocks=ESTIMATED_BLOCKS):
    """
    Calculate when first block hits reclaim threshold.

    Returns: tokens needed for first reclaim
    """
    # At N tokens: total_reads = read_rate √ó N
    # Max reads to hottest block ‚âà (total_reads / blocks) √ó concentration
    # Trigger when: max_reads ‚â• threshold

    # Solving: (read_rate √ó N / blocks) √ó concentration = threshold
    tokens_to_trigger = (threshold * blocks) / (read_rate * concentration)
    return tokens_to_trigger

def calculate_reclaim_frequency(threshold, tokens_total, read_rate=READ_RATE_PER_TOKEN,
                               concentration=10, blocks=ESTIMATED_BLOCKS):
    """
    Calculate how many times each hot block gets reclaimed during campaign.

    After first trigger, block is refreshed (read count resets).
    It accumulates reads again at same rate, triggers again, etc.
    """
    tokens_per_trigger = calculate_reclaim_trigger_time(threshold, read_rate, concentration, blocks)

    if tokens_per_trigger > tokens_total:
        return 0  # Never triggers

    # Number of times hottest blocks get reclaimed
    reclaims_per_hot_block = tokens_total / tokens_per_trigger
    return reclaims_per_hot_block

def calculate_pe_cycles_from_reclaim(threshold, tokens_total, hot_block_fraction=0.1):
    """
    Calculate P/E cycles consumed by read-reclaim.

    Args:
        threshold: Read count threshold for reclaim
        tokens_total: Total tokens generated in campaign
        hot_block_fraction: Fraction of blocks that are "hot" (default 10%)

    Returns:
        Average P/E cycles per block
    """
    num_hot_blocks = ESTIMATED_BLOCKS * hot_block_fraction
    reclaims_per_hot_block = calculate_reclaim_frequency(threshold, tokens_total)

    if reclaims_per_hot_block == 0:
        return 0

    # Each reclaim = 1 erase + reprogram = 1 P/E cycle
    total_pe_cycles = num_hot_blocks * reclaims_per_hot_block
    avg_pe_per_block = total_pe_cycles / ESTIMATED_BLOCKS

    return avg_pe_per_block

# ============================================================================
# RELIABILITY PROJECTION
# ============================================================================

def calculate_ecc_retry_rate(threshold, tokens_total):
    """
    Estimate ECC retry rate as function of accumulated reads.

    Higher threshold ‚Üí more accumulated reads ‚Üí higher BER ‚Üí more retries

    Simplified model: retry_rate ‚àù max_reads_accumulated
    """
    total_reads = READ_RATE_PER_TOKEN * tokens_total
    avg_reads, max_reads = estimate_max_reads_per_block(total_reads, ESTIMATED_BLOCKS)

    # If no reclaim: blocks accumulate up to max_reads
    # With reclaim at threshold: blocks reset, max accumulation = threshold
    max_accumulated = min(max_reads, threshold)

    # ECC retry rate increases with read count (read-disturb BER ‚àù reads^q)
    # Simplified: linear relationship for demonstration
    # Base failure rate at 0 reads, increases linearly
    base_failure_rate = ECC_FAILURE_RATE * 0.5  # baseline
    failure_rate = base_failure_rate + (ECC_FAILURE_RATE * max_accumulated / 1000)

    return failure_rate

# ============================================================================
# LIFETIME PROJECTION
# ============================================================================

def calculate_lifetime_years(avg_pe_per_block, tokens_per_day=1_000_000):
    """
    Project flash lifetime in years based on P/E consumption rate.

    Args:
        avg_pe_per_block: Average P/E cycles per block from reclaim
        tokens_per_day: Daily token generation (default 1M for high-usage)

    Returns:
        Lifetime in years
    """
    if avg_pe_per_block == 0:
        return float('inf')

    # P/E cycles consumed per day
    # (This is a simplification - real calculation more complex)
    pe_per_token = avg_pe_per_block  # assuming linear
    pe_per_day = pe_per_token * tokens_per_day

    # Days until P/E limit
    days_to_failure = PE_CYCLES_LIMIT / pe_per_day
    years_to_failure = days_to_failure / 365

    return years_to_failure

def calculate_tbw(tokens_total):
    """
    Calculate Total Bytes Written from reclaim operations.

    TBW = number of block migrations √ó block size
    """
    # This is what the user specifically asked about!
    # For each reclaim threshold, calculate total writes from migrations

    results = {}
    thresholds = [10, 50, 100, 500, 1000, 10000, 100000, 1000000]

    for threshold in thresholds:
        reclaims_per_hot = calculate_reclaim_frequency(threshold, tokens_total)
        hot_blocks = ESTIMATED_BLOCKS * 0.1  # 10% are hot

        total_reclaims = hot_blocks * reclaims_per_hot
        total_writes_mb = total_reclaims * BLOCK_SIZE_MB
        total_writes_gb = total_writes_mb / 1024
        total_writes_tb = total_writes_gb / 1024

        results[threshold] = {
            'reclaims': total_reclaims,
            'tbw': total_writes_tb,
            'pe_cycles': total_reclaims / ESTIMATED_BLOCKS  # avg per block
        }

    return results

# ============================================================================
# TRADE-OFF ANALYSIS
# ============================================================================

def analyze_tradeoff(tokens_campaign=10_000_000):  # 10M token campaign
    """
    Generate complete trade-off analysis.

    For each threshold, calculate:
    1. When reclaim would first trigger
    2. How many reclaims during campaign
    3. P/E cycles consumed
    4. ECC retry rate
    5. Projected lifetime
    """
    print("=" * 80)
    print(f"LLM READ-DISTURB TRADE-OFF ANALYSIS")
    print(f"Campaign: {tokens_campaign:,} tokens")
    print(f"Model: Llama2-70B (70GB, {ESTIMATED_BLOCKS:,} blocks)")
    print(f"Read Rate: {READ_RATE_PER_TOKEN} reads/token (measured, R¬≤={R_SQUARED})")
    print("=" * 80)
    print()

    # Test different thresholds
    thresholds = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000, 1000000]

    results = []

    for threshold in thresholds:
        # Calculate metrics
        tokens_to_trigger = calculate_reclaim_trigger_time(threshold)
        reclaims_per_hot = calculate_reclaim_frequency(threshold, tokens_campaign)
        avg_pe = calculate_pe_cycles_from_reclaim(threshold, tokens_campaign)
        retry_rate = calculate_ecc_retry_rate(threshold, tokens_campaign)
        lifetime_years = calculate_lifetime_years(avg_pe)

        # TBW calculation
        hot_blocks = ESTIMATED_BLOCKS * 0.1
        total_reclaims = hot_blocks * reclaims_per_hot
        tbw = (total_reclaims * BLOCK_SIZE_MB) / (1024 * 1024)  # TB

        results.append({
            'threshold': threshold,
            'tokens_to_trigger': tokens_to_trigger,
            'reclaims_per_hot_block': reclaims_per_hot,
            'avg_pe_cycles': avg_pe,
            'tbw': tbw,
            'ecc_failure_rate': retry_rate,
            'lifetime_years': lifetime_years
        })

    # Print table
    print(f"{'Threshold':<12} {'Trigger@':>12} {'Reclaims':>10} {'Avg P/E':>10} {'TBW (TB)':>10} {'Failure%':>10} {'Lifetime':>12}")
    print("-" * 80)

    for r in results:
        trigger_str = f"{r['tokens_to_trigger']/1e6:.1f}M tok" if r['tokens_to_trigger'] < 1e9 else ">1B tok"
        lifetime_str = f"{r['lifetime_years']:.1f} yr" if r['lifetime_years'] < 100 else ">100 yr"

        print(f"{r['threshold']:<12,} {trigger_str:>12} {r['reclaims_per_hot_block']:>10.1f} "
              f"{r['avg_pe_cycles']:>10.1f} {r['tbw']:>10.3f} {r['ecc_failure_rate']*100:>9.1f}% {lifetime_str:>12}")

    print()
    print("=" * 80)
    print("KEY FINDINGS:")
    print("=" * 80)

    # Find extremes
    low_threshold = results[0]
    high_threshold = results[-1]

    print(f"\nüî¥ LOW THRESHOLD ({low_threshold['threshold']} reads):")
    print(f"   ‚úì Low failure rate: {low_threshold['ecc_failure_rate']*100:.1f}%")
    print(f"   ‚úó High TBW: {low_threshold['tbw']:.2f} TB")
    print(f"   ‚úó High P/E cycles: {low_threshold['avg_pe_cycles']:.1f} per block")
    print(f"   ‚úó Short lifetime: {low_threshold['lifetime_years']:.1f} years")

    print(f"\nüîµ HIGH THRESHOLD ({high_threshold['threshold']:,} reads):")
    print(f"   ‚úó High failure rate: {high_threshold['ecc_failure_rate']*100:.1f}%")
    print(f"   ‚úì Low TBW: {high_threshold['tbw']:.2f} TB")
    print(f"   ‚úì Low P/E cycles: {high_threshold['avg_pe_cycles']:.1f} per block")
    if high_threshold['lifetime_years'] > 100:
        print(f"   ‚úì Long lifetime: >100 years")
    else:
        print(f"   ‚úì Long lifetime: {high_threshold['lifetime_years']:.1f} years")

    print(f"\n‚ö†Ô∏è  THE FUNDAMENTAL TRADE-OFF:")
    print(f"   ‚Üí Decreasing threshold by {high_threshold['threshold']/low_threshold['threshold']:.0f}√ó:")
    print(f"     ‚Ä¢ Reduces failures {high_threshold['ecc_failure_rate']/low_threshold['ecc_failure_rate']:.1f}√ó")
    print(f"     ‚Ä¢ BUT increases TBW {low_threshold['tbw']/max(high_threshold['tbw'], 0.001):.0f}√ó")
    print(f"     ‚Ä¢ AND reduces lifetime {high_threshold['lifetime_years']/max(low_threshold['lifetime_years'], 0.1):.0f}√ó")
    print(f"\n   ‚ö° NO SWEET SPOT EXISTS - opposing constraints!")

    print()
    print("=" * 80)

    return results

def generate_json_for_plotting(tokens_campaign=10_000_000, output_file=None):
    """Generate JSON data for plotting scripts."""
    thresholds = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000, 1000000]

    data = {
        'campaign_tokens': tokens_campaign,
        'model': 'Llama2-70B',
        'read_rate': READ_RATE_PER_TOKEN,
        'r_squared': R_SQUARED,
        'thresholds': []
    }

    for threshold in thresholds:
        reclaims_per_hot = calculate_reclaim_frequency(threshold, tokens_campaign)
        avg_pe = calculate_pe_cycles_from_reclaim(threshold, tokens_campaign)
        retry_rate = calculate_ecc_retry_rate(threshold, tokens_campaign)

        hot_blocks = ESTIMATED_BLOCKS * 0.1
        total_reclaims = hot_blocks * reclaims_per_hot
        tbw = (total_reclaims * BLOCK_SIZE_MB) / (1024 * 1024)

        data['thresholds'].append({
            'threshold': threshold,
            'reclaims_per_hot_block': reclaims_per_hot,
            'avg_pe_cycles': avg_pe,
            'tbw_tb': tbw,
            'ecc_failure_rate': retry_rate,
            'lifetime_years': calculate_lifetime_years(avg_pe)
        })

    if output_file:
        with open(output_file, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"\n‚úÖ Data exported to: {output_file}")

    return data

# ============================================================================
# MAIN
# ============================================================================

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Analytical trade-off analysis')
    parser.add_argument('-t', '--tokens', type=int, default=10_000_000,
                       help='Campaign length in tokens (default: 10M)')
    parser.add_argument('-o', '--output', help='Output JSON file for plotting')

    args = parser.parse_args()

    # Run analysis
    results = analyze_tradeoff(args.tokens)

    # Generate plotting data
    if args.output:
        generate_json_for_plotting(args.tokens, args.output)
