======================================================================
LLM Inference Simulation Results: llama7b_10000k_config_scenario_1
======================================================================

üìä Host I/O Statistics:
  Total Requests:      10,000
  Read Requests:       10,000
  IOPS:                91,051.91
  Bandwidth:           1490489159.17 MB/s
  Avg Response Time:   47635.00 Œºs

üîß Flash Operations:
  Flash Reads:         828 single + 4,550 multiplane
  Flash Writes:        0
  Flash Erases:        0
  GC Executions:       0
  Read-Reclaim Ops:    0

‚ö†Ô∏è  ECC Statistics (KEY METRICS):
  ECC Retries:         0
  ECC Failures:        18,117
  Uncorrectable:       18,117
  Retry Rate:          0.0000 (0.00%)
  Failure Rate:        3.3687 (336.87%)
  Uncorrectable Rate:  3.3687 (336.87%)

======================================================================
‚úÖ Exported metrics to /home/jwjeong/git_repo/MQSim-JW/results/exp1_baseline/llama7b_10000k.json
